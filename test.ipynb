{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2adc1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List, Union, Optional\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e4a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type aliases for enhanced clarity and Pylance compatibility\n",
    "ArrayLike = Union[np.ndarray, pd.Series]\n",
    "FloatArray = np.ndarray  # Explicit float array type\n",
    "PredictionArray = np.ndarray  # Standardized prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8edeb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Type-Safe Advanced CTR Prediction with Temporal Validation & Stacking\n",
      "=====================================================================================\n",
      "üìä Loading temporal training data with type validation...\n",
      "Loaded 1,000,000 rows...\n",
      "Loaded 2,000,000 rows...\n",
      "Loaded 3,000,000 rows...\n",
      "Loaded 4,000,000 rows...\n",
      "Loaded 5,000,000 rows...\n",
      "Loaded 6,000,000 rows...\n",
      "Loaded 7,000,000 rows...\n",
      "Loaded 8,000,000 rows...\n",
      "Loaded 9,000,000 rows...\n",
      "Loaded 10,000,000 rows...\n",
      "Loaded 11,000,000 rows...\n",
      "Loaded 12,000,000 rows...\n",
      "Loaded 13,000,000 rows...\n",
      "Loaded 14,000,000 rows...\n",
      "Loaded 15,000,000 rows...\n",
      "Loaded 16,000,000 rows...\n",
      "Loaded 17,000,000 rows...\n",
      "Loaded 18,000,000 rows...\n",
      "Loaded 19,000,000 rows...\n",
      "Loaded 20,000,000 rows...\n",
      "Loaded 21,000,000 rows...\n",
      "Loaded 22,000,000 rows...\n",
      "Loaded 23,000,000 rows...\n",
      "Loaded 24,000,000 rows...\n",
      "Loaded 25,000,000 rows...\n",
      "Loaded 26,000,000 rows...\n",
      "Loaded 27,000,000 rows...\n",
      "Loaded 28,000,000 rows...\n",
      "Loaded 29,000,000 rows...\n",
      "Loaded 30,000,000 rows...\n",
      "Loaded 31,000,000 rows...\n",
      "Loaded 32,000,000 rows...\n",
      "Loaded 33,000,000 rows...\n",
      "Loaded 34,000,000 rows...\n",
      "Loaded 35,000,000 rows...\n",
      "Loaded 36,000,000 rows...\n",
      "Loaded 37,000,000 rows...\n",
      "Loaded 38,000,000 rows...\n",
      "Loaded 39,000,000 rows...\n",
      "Loaded 40,000,000 rows...\n",
      "Loaded 40,388,935 rows...\n",
      "Training set shape: (40388935, 25)\n",
      "Temporal range: 14102100 - 14103023\n",
      "Base CTR: 0.1698 (16.98%)\n",
      "Class imbalance ratio: 4.9:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"üöÄ Type-Safe Advanced CTR Prediction with Temporal Validation & Stacking\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# =============================================\n",
    "# 1. TYPE-SAFE DATA LOADING WITH VALIDATION\n",
    "# =============================================\n",
    "\n",
    "def load_data_in_chunks(filepath: str, chunksize: int = 1000000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Type-safe chunked data loading with comprehensive validation\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to compressed CSV file\n",
    "        chunksize: Number of rows per chunk for memory optimization\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated DataFrame with validated structure\n",
    "    \"\"\"\n",
    "    chunks: List[pd.DataFrame] = []\n",
    "    total_rows: int = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunksize, compression='gzip'):\n",
    "        chunks.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "        print(f\"Loaded {total_rows:,} rows...\")\n",
    "            \n",
    "    consolidated_df: pd.DataFrame = pd.concat(chunks, ignore_index=True)\n",
    "    return consolidated_df\n",
    "\n",
    "print(\"üìä Loading temporal training data with type validation...\")\n",
    "train_df: pd.DataFrame = load_data_in_chunks('ctr_train.csv.gz')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Temporal range: {train_df['hour'].min()} - {train_df['hour'].max()}\")\n",
    "\n",
    "# Explicit type casting for numerical stability\n",
    "ctr: float = float(train_df['click'].mean())\n",
    "print(f\"Base CTR: {ctr:.4f} ({ctr*100:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {(1.0-ctr)/ctr:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7203bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 12 type-safe temporal features\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 2. ROBUST TEMPORAL FEATURE ENGINEERING\n",
    "# =============================================\n",
    "\n",
    "def create_comprehensive_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Type-safe comprehensive temporal feature engineering with cyclical encoding\n",
    "    and interaction terms optimized for CTR prediction tasks\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with 'hour' column in YYMMDDHH format\n",
    "        \n",
    "    Returns:\n",
    "        Enhanced DataFrame with engineered temporal features\n",
    "    \"\"\"\n",
    "    df_enhanced: pd.DataFrame = df.copy()\n",
    "    \n",
    "    # Primary temporal decomposition with explicit int conversion\n",
    "    hour_series: pd.Series = df_enhanced['hour'].astype(np.int64)\n",
    "    df_enhanced['hour_of_day'] = (hour_series % 100).astype(np.int32)\n",
    "    df_enhanced['day'] = ((hour_series // 100) % 100).astype(np.int32)\n",
    "    df_enhanced['month'] = ((hour_series // 10000) % 100).astype(np.int32)\n",
    "    df_enhanced['year'] = (hour_series // 1000000).astype(np.int32)\n",
    "    \n",
    "    # Direct day of week calculation (assumes correct format)\n",
    "    datetime_series = pd.to_datetime(hour_series.astype(str), format='%y%m%d%H')\n",
    "    df_enhanced['day_of_week'] = datetime_series.dt.dayofweek.astype(np.int32)\n",
    "    \n",
    "    # Cyclical encoding for temporal periodicity preservation\n",
    "    hour_of_day_float: FloatArray = df_enhanced['hour_of_day'].astype(np.float64).to_numpy()\n",
    "    day_of_week_float: FloatArray = df_enhanced['day_of_week'].astype(np.float64).to_numpy()\n",
    "    \n",
    "    df_enhanced['hour_sin'] = np.sin(2.0 * np.pi * hour_of_day_float / 24.0).astype(np.float32)\n",
    "    df_enhanced['hour_cos'] = np.cos(2.0 * np.pi * hour_of_day_float / 24.0).astype(np.float32)\n",
    "    df_enhanced['dow_sin'] = np.sin(2.0 * np.pi * day_of_week_float / 7.0).astype(np.float32)\n",
    "    df_enhanced['dow_cos'] = np.cos(2.0 * np.pi * day_of_week_float / 7.0).astype(np.float32)\n",
    "    \n",
    "    # Business logic features with explicit boolean conversion\n",
    "    weekend_mask: pd.Series = (df_enhanced['day_of_week'] >= 5)\n",
    "    business_hour_mask: pd.Series = (\n",
    "        (df_enhanced['hour_of_day'] >= 9) & \n",
    "        (df_enhanced['hour_of_day'] <= 17) &\n",
    "        (~weekend_mask)\n",
    "    )\n",
    "    \n",
    "    df_enhanced['is_weekend'] = weekend_mask.astype(np.int8)\n",
    "    df_enhanced['is_business_hour'] = business_hour_mask.astype(np.int8)\n",
    "    \n",
    "    # Time period categorization with robust binning\n",
    "    hour_bins: List[float] = [-0.1, 6.0, 12.0, 18.0, 24.0]\n",
    "    hour_labels: List[int] = [0, 1, 2, 3]  # night, morning, day, evening\n",
    "    \n",
    "    df_enhanced['time_period'] = pd.cut(\n",
    "        hour_of_day_float, \n",
    "        bins=hour_bins, \n",
    "        labels=hour_labels,\n",
    "        include_lowest=True\n",
    "    ).fillna(0).astype(np.int8)\n",
    "    \n",
    "    new_features: int = len([c for c in df_enhanced.columns if c not in df.columns])\n",
    "    print(f\"‚úÖ Created {new_features} type-safe temporal features\")\n",
    "    return df_enhanced\n",
    "\n",
    "# Apply temporal feature engineering with type safety\n",
    "train_df = create_comprehensive_temporal_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc067d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. TYPE-SAFE FREQUENCY ENCODING IMPLEMENTATION\n",
    "# =============================================\n",
    "\n",
    "def frequency_encoding_with_smoothing(\n",
    "    train_df: pd.DataFrame, \n",
    "    val_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame,\n",
    "    high_card_cols: List[str], \n",
    "    smoothing_factor: float = 10.0\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Type-safe frequency encoding with Laplace smoothing for high-cardinality\n",
    "    categorical features, preventing overfitting on rare categories\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        val_df: Validation DataFrame  \n",
    "        test_df: Test DataFrame\n",
    "        high_card_cols: List of high-cardinality column names\n",
    "        smoothing_factor: Laplace smoothing parameter (Œ±)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of enhanced DataFrames with frequency-encoded features\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Applying type-safe frequency encoding with Œ±={smoothing_factor} smoothing...\")\n",
    "    \n",
    "    encoded_dfs: Dict[str, pd.DataFrame] = {\n",
    "        'train': train_df.copy(), \n",
    "        'val': val_df.copy(), \n",
    "        'test': test_df.copy()\n",
    "    }\n",
    "    \n",
    "    for col in high_card_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Type-safe frequency computation on training data only\n",
    "        col_series: pd.Series = train_df[col].astype(str)\n",
    "        freq_map: Dict[str, int] = col_series.value_counts().to_dict()\n",
    "        total_count: int = len(train_df)\n",
    "        vocab_size: int = len(freq_map)\n",
    "        \n",
    "        # Laplace smoothing with explicit float conversion\n",
    "        def smooth_frequency(value: str) -> float:\n",
    "            raw_freq: int = freq_map.get(str(value), 0)\n",
    "            smoothed: float = (float(raw_freq) + smoothing_factor) / (\n",
    "                float(total_count) + smoothing_factor * float(vocab_size)\n",
    "            )\n",
    "            return smoothed\n",
    "        \n",
    "        # Default frequency for unseen values\n",
    "        unseen_freq: float = smoothing_factor / (\n",
    "            float(total_count) + smoothing_factor * float(vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Apply frequency encoding to all datasets with type consistency\n",
    "        for df_name, df in encoded_dfs.items():\n",
    "            col_values: pd.Series = df[col].astype(str)\n",
    "            frequency_array: FloatArray = np.array([\n",
    "                smooth_frequency(val) for val in col_values\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            encoded_dfs[df_name][f'{col}_freq'] = frequency_array\n",
    "            \n",
    "        print(f\"  {col}: {vocab_size:,} unique values ‚Üí frequency encoded\")\n",
    "    \n",
    "    return encoded_dfs['train'], encoded_dfs['val'], encoded_dfs['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722211bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. TYPE-SAFE CTR AGGREGATION FEATURES\n",
    "# =============================================\n",
    "\n",
    "def create_ctr_aggregation_features(\n",
    "    train_df: pd.DataFrame, \n",
    "    val_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame,\n",
    "    target_col: str = 'click', \n",
    "    min_samples: int = 50\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create type-safe historical CTR aggregation features with temporal safety\n",
    "    to prevent data leakage while capturing categorical-target interactions\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        val_df: Validation DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        target_col: Name of binary target column\n",
    "        min_samples: Minimum samples required for reliable CTR estimation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of DataFrames enhanced with CTR-based features\n",
    "    \"\"\"\n",
    "    print(f\"üìà Creating type-safe CTR aggregation features (min_samples={min_samples})...\")\n",
    "    print(f\"  Train temporal range: {train_df['hour'].min()} - {train_df['hour'].max()}\")\n",
    "    print(f\"  Val temporal range: {val_df['hour'].min()} - {val_df['hour'].max()}\")\n",
    "    print(f\"  Test temporal range: {test_df['hour'].min()} - {test_df['hour'].max()}\")\n",
    "    \n",
    "    categorical_cols: List[str] = [\n",
    "        'site_category', 'app_category', 'device_type', 'device_conn_type',\n",
    "        'banner_pos', 'hour_of_day', 'day_of_week', 'time_period'\n",
    "    ]\n",
    "    \n",
    "    train_enhanced: pd.DataFrame = train_df.copy()\n",
    "    val_enhanced: pd.DataFrame = val_df.copy()\n",
    "    test_enhanced: pd.DataFrame = test_df.copy()\n",
    "    \n",
    "    # Global CTR with explicit float conversion\n",
    "    global_ctr: float = float(train_df[target_col].mean())\n",
    "    print(f\"  Global CTR baseline: {global_ctr:.4f}\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Type-safe CTR statistics computation\n",
    "        ctr_stats: pd.DataFrame = (\n",
    "            train_df.groupby(col)[target_col]\n",
    "            .agg(['count', 'mean', 'std'])\n",
    "            .reset_index()\n",
    "        )\n",
    "        ctr_stats.columns = [col, f'{col}_count', f'{col}_ctr', f'{col}_ctr_std']\n",
    "        \n",
    "        # Filter by minimum sample requirement with type safety\n",
    "        count_mask: pd.Series = ctr_stats[f'{col}_count'] >= min_samples\n",
    "        reliable_stats: pd.DataFrame = ctr_stats[count_mask].copy()\n",
    "        \n",
    "        # Create type-safe mapping dictionaries\n",
    "        ctr_map: Dict[Union[str, int, float], float] = dict(\n",
    "            zip(reliable_stats[col], reliable_stats[f'{col}_ctr'].astype(float))\n",
    "        )\n",
    "        count_map: Dict[Union[str, int, float], int] = dict(\n",
    "            zip(reliable_stats[col], reliable_stats[f'{col}_count'].astype(int))\n",
    "        )\n",
    "        std_map: Dict[Union[str, int, float], float] = dict(\n",
    "            zip(reliable_stats[col], reliable_stats[f'{col}_ctr_std'].fillna(0.0).astype(float))\n",
    "        )\n",
    "        \n",
    "        # Apply to all datasets with explicit type handling\n",
    "        datasets: List[Tuple[pd.DataFrame, str]] = [\n",
    "            (train_enhanced, 'train'), \n",
    "            (val_enhanced, 'val'), \n",
    "            (test_enhanced, 'test')\n",
    "        ]\n",
    "        \n",
    "        for df, df_name in datasets:\n",
    "            # Type-safe mapping with fallback handling\n",
    "            ctr_values: FloatArray = np.array([\n",
    "                ctr_map.get(val, global_ctr) for val in df[col]\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            count_values: np.ndarray = np.array([\n",
    "                count_map.get(val, 0) for val in df[col]\n",
    "            ], dtype=np.int32)\n",
    "            \n",
    "            std_values: FloatArray = np.array([\n",
    "                std_map.get(val, 0.0) for val in df[col]\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            df[f'{col}_historical_ctr'] = ctr_values\n",
    "            df[f'{col}_sample_count'] = count_values\n",
    "            df[f'{col}_ctr_std'] = std_values\n",
    "            \n",
    "            # Confidence score with logarithmic scaling\n",
    "            df[f'{col}_ctr_confidence'] = np.log1p(count_values.astype(np.float32))\n",
    "        \n",
    "        reliable_categories: int = len(reliable_stats)\n",
    "        total_categories: int = len(ctr_stats)\n",
    "        print(f\"  {col}: {reliable_categories}/{total_categories} categories with sufficient samples\")\n",
    "    \n",
    "    return train_enhanced, val_enhanced, test_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0b563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïí IMPLEMENTING TYPE-SAFE TEMPORAL DATA SPLITTING\n",
      "=======================================================\n",
      "üìä Temporal split verification:\n",
      "  Training: 14102100 ‚Üí 14102823\n",
      "  Validation: 14102823 ‚Üí 14103023\n",
      "  Temporal gap: ‚úÖ No leakage\n",
      "üìà CTR distribution stability:\n",
      "  Training CTR: 0.1715\n",
      "  Validation CTR: 0.1632\n",
      "  Relative difference: 4.84%\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 5. TYPE-SAFE TEMPORAL DATA SPLITTING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüïí IMPLEMENTING TYPE-SAFE TEMPORAL DATA SPLITTING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Sort by temporal order with explicit type validation\n",
    "train_df_sorted: pd.DataFrame = train_df.sort_values('hour').reset_index(drop=True)\n",
    "\n",
    "# Temporal split with precise indexing\n",
    "split_idx: int = int(len(train_df_sorted) * 0.8)\n",
    "\n",
    "train_temporal: pd.DataFrame = train_df_sorted.iloc[:split_idx].copy()\n",
    "val_temporal: pd.DataFrame = train_df_sorted.iloc[split_idx:].copy()\n",
    "\n",
    "# Type-safe temporal integrity verification\n",
    "train_max_time: int = int(train_temporal['hour'].max())\n",
    "val_min_time: int = int(val_temporal['hour'].min())\n",
    "\n",
    "print(f\"üìä Temporal split verification:\")\n",
    "print(f\"  Training: {train_temporal['hour'].min()} ‚Üí {train_max_time}\")\n",
    "print(f\"  Validation: {val_min_time} ‚Üí {val_temporal['hour'].max()}\")\n",
    "leakage_status: str = '‚úÖ No leakage' if train_max_time <= val_min_time else '‚ùå LEAKAGE DETECTED'\n",
    "print(f\"  Temporal gap: {leakage_status}\")\n",
    "\n",
    "# CTR distribution analysis with type safety\n",
    "train_ctr: float = float(train_temporal['click'].mean())\n",
    "val_ctr: float = float(val_temporal['click'].mean())\n",
    "relative_diff: float = abs(train_ctr - val_ctr) / train_ctr * 100.0\n",
    "\n",
    "print(f\"üìà CTR distribution stability:\")\n",
    "print(f\"  Training CTR: {train_ctr:.4f}\")\n",
    "print(f\"  Validation CTR: {val_ctr:.4f}\")\n",
    "print(f\"  Relative difference: {relative_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f455d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading test data with type-safe preprocessing...\n",
      "‚úÖ Created 7 type-safe temporal features\n",
      "Test set shape: (40032, 37)\n",
      "Test temporal range: 14102100 ‚Üí 14102101\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 6. TYPE-SAFE TEST DATA LOADING AND PREPROCESSING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüì• Loading test data with type-safe preprocessing...\")\n",
    "test_df: pd.DataFrame = pd.read_csv('ctr_test.csv')\n",
    "test_df = create_comprehensive_temporal_features(test_df)\n",
    "\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Test temporal range: {test_df['hour'].min()} ‚Üí {test_df['hour'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b02c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Applying type-safe frequency encoding with Œ±=10.0 smoothing...\n",
      "  device_id: 2,215,224 unique values ‚Üí frequency encoded\n",
      "  site_id: 4,550 unique values ‚Üí frequency encoded\n",
      "  device_ip: 5,559,975 unique values ‚Üí frequency encoded\n",
      "  app_id: 8,092 unique values ‚Üí frequency encoded\n",
      "  device_model: 8,034 unique values ‚Üí frequency encoded\n",
      "üìà Creating type-safe CTR aggregation features (min_samples=50)...\n",
      "  Train temporal range: 14102100 - 14102823\n",
      "  Val temporal range: 14102823 - 14103023\n",
      "  Test temporal range: 14102100 - 14102101\n",
      "  Global CTR baseline: 0.1715\n",
      "  site_category: 20/26 categories with sufficient samples\n",
      "  app_category: 25/36 categories with sufficient samples\n",
      "  device_type: 4/5 categories with sufficient samples\n",
      "  device_conn_type: 4/4 categories with sufficient samples\n",
      "  banner_pos: 7/7 categories with sufficient samples\n",
      "  hour_of_day: 24/24 categories with sufficient samples\n",
      "  day_of_week: 7/7 categories with sufficient samples\n",
      "  time_period: 4/4 categories with sufficient samples\n",
      "\n",
      "üìè Final feature matrix dimensions:\n",
      "  Training: (32311148, 74)\n",
      "  Validation: (8077787, 74)\n",
      "  Test: (40032, 74)\n",
      "üìã Feature preparation complete:\n",
      "  Total features: 70\n",
      "  Categorical features: 29\n",
      "  Numerical features: 41\n",
      "üß† Memory cleanup completed. Training on 32,311,148 samples.\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 7. FEATURE ENGINEERING PIPELINE EXECUTION\n",
    "# =============================================\n",
    "\n",
    "# High-cardinality categorical features for frequency encoding\n",
    "high_cardinality_features: List[str] = ['device_id', 'site_id', 'device_ip', 'app_id', 'device_model']\n",
    "\n",
    "# Apply frequency encoding with type safety\n",
    "train_freq, val_freq, test_freq = frequency_encoding_with_smoothing(\n",
    "    train_temporal, val_temporal, test_df, \n",
    "    high_cardinality_features, \n",
    "    smoothing_factor=10.0\n",
    ")\n",
    "\n",
    "# Apply CTR aggregation features with type safety\n",
    "train_final, val_final, test_final = create_ctr_aggregation_features(\n",
    "    train_freq, val_freq, test_freq\n",
    ")\n",
    "\n",
    "print(f\"\\nüìè Final feature matrix dimensions:\")\n",
    "print(f\"  Training: {train_final.shape}\")\n",
    "print(f\"  Validation: {val_final.shape}\")\n",
    "print(f\"  Test: {test_final.shape}\")\n",
    "\n",
    "# Prepare model features\n",
    "def prepare_model_features(train_df, val_df, test_df):\n",
    "    exclude_cols = {'idx', 'id', 'click', 'hour'}\n",
    "    feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_df.loc[:, feature_cols]\n",
    "    y_train = train_df['click']\n",
    "    \n",
    "    X_val = val_df.loc[:, feature_cols]\n",
    "    y_val = val_df['click']\n",
    "    \n",
    "    X_test = test_df.loc[:, feature_cols]\n",
    "\n",
    "    dtypes = X_train.dtypes.to_dict()\n",
    "    nunique_map = X_train.nunique(dropna=False).to_dict()\n",
    "    \n",
    "    engineered_suffixes = ('_freq', '_ctr', '_std', '_confidence', '_count', '_sin', '_cos')\n",
    "    always_categorical = {'is_weekend', 'is_business_hour', 'time_period'}\n",
    "\n",
    "    categorical_features = []\n",
    "    threshold = 0.5 * len(X_train)\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if col.endswith(engineered_suffixes):\n",
    "            continue\n",
    "        if col in always_categorical:\n",
    "            categorical_features.append(col)\n",
    "        elif dtypes[col] == 'object' or nunique_map[col] < threshold:\n",
    "            categorical_features.append(col)\n",
    "\n",
    "    print(f\"üìã Feature preparation complete:\")\n",
    "    print(f\"  Total features: {len(feature_cols)}\")\n",
    "    print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"  Numerical features: {len(feature_cols) - len(categorical_features)}\")\n",
    "\n",
    "    for df in (X_train, X_val, X_test):\n",
    "        for col in categorical_features:\n",
    "            if df[col].dtype.name != 'category':\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, categorical_features\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, categorical_features = prepare_model_features(\n",
    "    train_final, val_final, test_final\n",
    ")\n",
    "\n",
    "# Memory cleanup\n",
    "to_delete = [\n",
    "    'train_df', 'train_df_sorted', 'train_temporal', 'val_temporal',\n",
    "    'train_freq', 'val_freq', 'test_freq', 'train_final', 'val_final'\n",
    "]\n",
    "\n",
    "for var in to_delete:\n",
    "    globals()[var] = None\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(f\"üß† Memory cleanup completed. Training on {len(X_train):,} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cc0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü TYPE-SAFE 5-FOLD CROSS-VALIDATION AND BASE MODEL TRAINING\n",
      "============================================================\n",
      "üîß Performing 5-fold cross-validation for base models...\n",
      "\n",
      "Fold 1/5\n",
      "  LightGBM: Testing num_leaves=63, learning_rate=0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m     99\u001b[39m train_lgb = lgb.Dataset(\n\u001b[32m    100\u001b[39m     X_fold_train,\n\u001b[32m    101\u001b[39m     label=y_fold_train.astype(np.int8),\n\u001b[32m    102\u001b[39m     categorical_feature=categorical_features,\n\u001b[32m    103\u001b[39m     free_raw_data=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    104\u001b[39m )\n\u001b[32m    105\u001b[39m val_lgb = lgb.Dataset(\n\u001b[32m    106\u001b[39m     X_fold_val,\n\u001b[32m    107\u001b[39m     label=y_fold_val.astype(np.int8),\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     free_raw_data=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    111\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m lgb_model_tune = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlgb_params_tune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_lgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_lgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_lgb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m lgb_val_pred_tune = np.asarray(lgb_model_tune.predict(X_fold_val, num_iteration=lgb_model_tune.best_iteration), dtype=np.float64)\n\u001b[32m    126\u001b[39m val_auc = roc_auc_score(y_fold_val, lgb_val_pred_tune)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLS/.venv/lib/python3.11/site-packages/lightgbm/engine.py:328\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m         evaluation_result_list.extend(\u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    329\u001b[39m     evaluation_result_list.extend(booster.eval_valid(feval))\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLS/.venv/lib/python3.11/site-packages/lightgbm/basic.py:4408\u001b[39m, in \u001b[36mBooster.eval_train\u001b[39m\u001b[34m(self, feval)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_train\u001b[39m(\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4378\u001b[39m     feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4379\u001b[39m ) -> List[_LGBM_BoosterEvalMethodResultType]:\n\u001b[32m   4380\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate for training data.\u001b[39;00m\n\u001b[32m   4381\u001b[39m \n\u001b[32m   4382\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4406\u001b[39m \u001b[33;03m        List with (train_dataset_name, eval_name, eval_result, is_higher_better) tuples.\u001b[39;00m\n\u001b[32m   4407\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_data_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLS/.venv/lib/python3.11/site-packages/lightgbm/basic.py:5185\u001b[39m, in \u001b[36mBooster.__inner_eval\u001b[39m\u001b[34m(self, data_name, data_idx, feval)\u001b[39m\n\u001b[32m   5182\u001b[39m result = np.empty(\u001b[38;5;28mself\u001b[39m.__num_inner_eval, dtype=np.float64)\n\u001b[32m   5183\u001b[39m tmp_out_len = ctypes.c_int(\u001b[32m0\u001b[39m)\n\u001b[32m   5184\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m5185\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterGetEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5186\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_out_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5191\u001b[39m )\n\u001b[32m   5192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tmp_out_len.value != \u001b[38;5;28mself\u001b[39m.__num_inner_eval:\n\u001b[32m   5193\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length of eval results\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 9. TYPE-SAFE 5-FOLD CROSS-VALIDATION AND BASE MODEL TRAINING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüåü TYPE-SAFE 5-FOLD CROSS-VALIDATION AND BASE MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate class balance\n",
    "neg_count: int = int((y_train == 0).sum())\n",
    "pos_count: int = int((y_train == 1).sum())\n",
    "scale_pos_weight: float = float(neg_count) / float(pos_count)\n",
    "\n",
    "# Base LightGBM parameters\n",
    "lgb_base_params: Dict[str, Union[str, int, float, bool]] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 100,\n",
    "    'min_gain_to_split': 0.02,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 255,\n",
    "    'num_threads': 4,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Base CatBoost parameters\n",
    "catboost_base_params: Dict[str, Union[int, float, str, bool]] = {\n",
    "    'iterations': 2000,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'eval_metric': 'AUC',\n",
    "    'loss_function': 'Logloss',\n",
    "    'random_seed': 42,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'use_best_model': True,\n",
    "    'task_type': 'CPU',\n",
    "    'thread_count': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Hyperparameter grids\n",
    "lgb_param_grid = {\n",
    "    'num_leaves': [63, 127],\n",
    "    'learning_rate': [0.01, 0.02]\n",
    "}\n",
    "\n",
    "catboost_param_grid = {\n",
    "    'depth': [6, 8],\n",
    "    'learning_rate': [0.01, 0.02]\n",
    "}\n",
    "\n",
    "# Initialize arrays for out-of-fold predictions\n",
    "lgb_oof_pred = np.zeros(len(X_train), dtype=np.float64)\n",
    "catboost_oof_pred = np.zeros(len(X_train), dtype=np.float64)\n",
    "val_oof_pred = np.zeros(len(X_val), dtype=np.float64)\n",
    "lgb_test_pred = np.zeros(len(X_test), dtype=np.float64)\n",
    "catboost_test_pred = np.zeros(len(X_test), dtype=np.float64)\n",
    "\n",
    "# 5-fold cross-validation with TimeSeriesSplit\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "print(\"üîß Performing 5-fold cross-validation for base models...\")\n",
    "\n",
    "# Store best parameters for final models\n",
    "best_lgb_params: Optional[Dict[str, Union[str, int, float, bool]]] = None\n",
    "best_lgb_val_auc: float = 0.0\n",
    "best_catboost_params: Optional[Dict[str, Union[int, float, str, bool]]] = None\n",
    "best_catboost_val_auc: float = 0.0\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    print(f\"\\nFold {fold}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train = X_train.iloc[train_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx]\n",
    "    X_fold_val = X_train.iloc[val_idx]\n",
    "    y_fold_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    # LightGBM tuning\n",
    "    fold_best_lgb_params = None\n",
    "    fold_best_lgb_val_auc = 0.0\n",
    "    \n",
    "    for num_leaves in lgb_param_grid['num_leaves']:\n",
    "        for learning_rate in lgb_param_grid['learning_rate']:\n",
    "            print(f\"  LightGBM: Testing num_leaves={num_leaves}, learning_rate={learning_rate}\")\n",
    "            lgb_params_tune = lgb_base_params.copy()\n",
    "            lgb_params_tune['num_leaves'] = num_leaves\n",
    "            lgb_params_tune['learning_rate'] = learning_rate\n",
    "            \n",
    "            train_lgb = lgb.Dataset(\n",
    "                X_fold_train,\n",
    "                label=y_fold_train.astype(np.int8),\n",
    "                categorical_feature=categorical_features,\n",
    "                free_raw_data=True\n",
    "            )\n",
    "            val_lgb = lgb.Dataset(\n",
    "                X_fold_val,\n",
    "                label=y_fold_val.astype(np.int8),\n",
    "                categorical_feature=categorical_features,\n",
    "                reference=train_lgb,\n",
    "                free_raw_data=True\n",
    "            )\n",
    "            \n",
    "            lgb_model_tune = lgb.train(\n",
    "                lgb_params_tune,\n",
    "                train_lgb,\n",
    "                valid_sets=[train_lgb, val_lgb],\n",
    "                valid_names=['train', 'eval'],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                    lgb.log_evaluation(period=0)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            lgb_val_pred_tune = np.asarray(lgb_model_tune.predict(X_fold_val, num_iteration=lgb_model_tune.best_iteration), dtype=np.float64)\n",
    "            val_auc = roc_auc_score(y_fold_val, lgb_val_pred_tune)\n",
    "            print(f\"    Validation AUC: {val_auc:.6f}\")\n",
    "            \n",
    "            if val_auc > fold_best_lgb_val_auc:\n",
    "                fold_best_lgb_val_auc = val_auc\n",
    "                fold_best_lgb_params = lgb_params_tune\n",
    "    \n",
    "    if fold_best_lgb_params is None:\n",
    "        raise ValueError(f\"No best LightGBM parameters found for fold {fold}\")\n",
    "    \n",
    "    # Update global best if better\n",
    "    if fold_best_lgb_val_auc > best_lgb_val_auc:\n",
    "        best_lgb_val_auc = float(fold_best_lgb_val_auc)\n",
    "        best_lgb_params = fold_best_lgb_params\n",
    "    \n",
    "    # Train LightGBM with best fold parameters for OOF predictions\n",
    "    train_lgb = lgb.Dataset(\n",
    "        X_fold_train,\n",
    "        label=y_fold_train.astype(np.int8),\n",
    "        categorical_feature=categorical_features,\n",
    "        free_raw_data=True\n",
    "    )\n",
    "    val_lgb = lgb.Dataset(\n",
    "        X_fold_val,\n",
    "        label=y_fold_val.astype(np.int8),\n",
    "        categorical_feature=categorical_features,\n",
    "        reference=train_lgb,\n",
    "        free_raw_data=True\n",
    "    )\n",
    "    \n",
    "    lgb_model = lgb.train(\n",
    "        fold_best_lgb_params,\n",
    "        train_lgb,\n",
    "        valid_sets=[train_lgb, val_lgb],\n",
    "        valid_names=['train', 'eval'],\n",
    "        num_boost_round=1400,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lgb_oof_pred[val_idx] = np.asarray(lgb_model.predict(X_fold_val, num_iteration=lgb_model.best_iteration), dtype=np.float64)\n",
    "    lgb_test_pred += np.asarray(lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration), dtype=np.float64) / n_splits\n",
    "    \n",
    "    # CatBoost tuning\n",
    "    fold_best_catboost_params = None\n",
    "    fold_best_catboost_val_auc = 0.0\n",
    "    \n",
    "    for depth in catboost_param_grid['depth']:\n",
    "        for learning_rate in catboost_param_grid['learning_rate']:\n",
    "            print(f\"  CatBoost: Testing depth={depth}, learning_rate={learning_rate}\")\n",
    "            catboost_params_tune = catboost_base_params.copy()\n",
    "            catboost_params_tune['depth'] = depth\n",
    "            catboost_params_tune['learning_rate'] = learning_rate\n",
    "            \n",
    "            catboost_model_tune = CatBoostClassifier(**catboost_params_tune)\n",
    "            catboost_model_tune.fit(\n",
    "                X_fold_train, y_fold_train,\n",
    "                cat_features=categorical_features,\n",
    "                eval_set=(X_fold_val, y_fold_val),\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            catboost_val_pred_tune = np.asarray(catboost_model_tune.predict_proba(X_fold_val)[:, 1], dtype=np.float64)\n",
    "            val_auc = roc_auc_score(y_fold_val, catboost_val_pred_tune)\n",
    "            print(f\"    Validation AUC: {val_auc:.6f}\")\n",
    "            \n",
    "            if val_auc > fold_best_catboost_val_auc:\n",
    "                fold_best_catboost_val_auc = val_auc\n",
    "                fold_best_catboost_params = catboost_params_tune\n",
    "    \n",
    "    if fold_best_catboost_params is None:\n",
    "        raise ValueError(f\"No best CatBoost parameters found for fold {fold}\")\n",
    "    \n",
    "    # Update global best if better\n",
    "    if fold_best_catboost_val_auc > best_catboost_val_auc:\n",
    "        best_catboost_val_auc = float(fold_best_catboost_val_auc)\n",
    "        best_catboost_params = fold_best_catboost_params\n",
    "    \n",
    "    # Train CatBoost with best fold parameters for OOF predictions\n",
    "    catboost_model = CatBoostClassifier(**fold_best_catboost_params)\n",
    "    catboost_model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        cat_features=categorical_features,\n",
    "        eval_set=(X_fold_val, y_fold_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    catboost_oof_pred[val_idx] = np.asarray(catboost_model.predict_proba(X_fold_val)[:, 1], dtype=np.float64)\n",
    "    catboost_test_pred += np.asarray(catboost_model.predict_proba(X_test)[:, 1], dtype=np.float64) / n_splits\n",
    "\n",
    "# Generate validation predictions for meta-model\n",
    "# (Moved to after final base model training to ensure lgb_model and catboost_model are defined)\n",
    "# lgb_val_pred = np.asarray(lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration), dtype=np.float64)\n",
    "# catboost_val_pred = np.asarray(catboost_model.predict_proba(X_val)[:, 1], dtype=np.float64)\n",
    "# val_oof_pred = np.column_stack((lgb_val_pred, catboost_val_pred))\n",
    "\n",
    "# Memory cleanup after cross-validation\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26174337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training data: (32311148, 70)\n",
      "\n",
      "üåü TYPE-SAFE 5-FOLD CV + BASE MODELS TRAINING\n",
      "============================================================\n",
      "\n",
      "üîÅ Fold 1/5\n",
      "üü© LightGBM training...\n",
      "[100]\ttrain's auc: 0.746482\tval's auc: 0.74535\n",
      "[200]\ttrain's auc: 0.752802\tval's auc: 0.75125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time, gc\n",
    "from typing import Dict, Union, Optional\n",
    "\n",
    "# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ===\n",
    "n_splits = 5  # –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "sample_frac = 1.0  # —Ä–∞–±–æ—Ç–∞–µ–º —Å –ø–æ–ª–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º\n",
    "\n",
    "# üß™ –ü–æ–¥—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "X_train_sample = X_train.sample(frac=sample_frac, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "print(f\"‚úÖ Training data: {X_train_sample.shape}\")\n",
    "\n",
    "# ‚öñÔ∏è –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    "neg_count = int((y_train_sample == 0).sum())\n",
    "pos_count = int((y_train_sample == 1).sum())\n",
    "scale_pos_weight = float(neg_count) / float(pos_count)\n",
    "\n",
    "# üí° –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è CatBoost\n",
    "categorical_features = X_train_sample.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "le_dict = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    str_values = X_train_sample[col].astype(str).tolist()\n",
    "    encoded_values = le.fit_transform(str_values)\n",
    "    encoded_values = np.asarray(encoded_values, dtype=np.int32)\n",
    "    X_train_sample[col] = encoded_values\n",
    "    le_dict[col] = le\n",
    "\n",
    "# üéØ –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ float –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫ float32\n",
    "float_cols = X_train_sample.select_dtypes(include=['float64']).columns\n",
    "X_train_sample[float_cols] = X_train_sample[float_cols].astype(np.float32)\n",
    "\n",
    "# üìä LightGBM –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "lgb_base_params: Dict[str, Union[str, int, float, bool]] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 100,\n",
    "    'min_gain_to_split': 0.02,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 255,\n",
    "    'num_threads': 4,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# üê± CatBoost –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "catboost_base_params: Dict[str, Union[int, float, str, bool]] = {\n",
    "    'iterations': 1000,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'eval_metric': 'AUC',\n",
    "    'loss_function': 'Logloss',\n",
    "    'random_seed': 42,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'use_best_model': True,\n",
    "    'task_type': 'CPU',\n",
    "    'thread_count': 4,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# üì¶ OOF –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "lgb_oof_pred = np.zeros(len(X_train_sample), dtype=np.float32)\n",
    "catboost_oof_pred = np.zeros(len(X_train_sample), dtype=np.float32)\n",
    "\n",
    "# ‚öôÔ∏è –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "print(\"\\nüåü TYPE-SAFE 5-FOLD CV + BASE MODELS TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å\n",
    "best_lgb_auc, best_cat_auc = 0.0, 0.0\n",
    "best_lgb_params, best_cat_params = None, None\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_sample), 1):\n",
    "    print(f\"\\nüîÅ Fold {fold}/{n_splits}\")\n",
    "    fold_start = time.time()\n",
    "\n",
    "    X_tr, X_val = X_train_sample.iloc[train_idx].copy(), X_train_sample.iloc[val_idx].copy()\n",
    "    y_tr, y_val = y_train_sample.iloc[train_idx], y_train_sample.iloc[val_idx]\n",
    "\n",
    "    # === LIGHTGBM ===\n",
    "    print(\"üü© LightGBM training...\")\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr.astype(float))\n",
    "    lgb_valid = lgb.Dataset(X_val, label=y_val.astype(float))\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        lgb_base_params,\n",
    "        lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50, verbose=False),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
    "    pred = np.asarray(pred, dtype=np.float32).ravel()\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    print(f\"    ‚úÖ LightGBM AUC: {auc:.5f} | Time: {time.time() - fold_start:.1f}s\")\n",
    "\n",
    "    if auc > best_lgb_auc:\n",
    "        best_lgb_auc = auc\n",
    "        best_lgb_params = lgb_base_params.copy()\n",
    "\n",
    "    lgb_oof_pred[val_idx] = pred\n",
    "\n",
    "    # === CATBOOST ===\n",
    "    print(\"üê± CatBoost training...\")\n",
    "    cat_model = CatBoostClassifier(**catboost_base_params)\n",
    "    cat_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=(X_val, y_val),\n",
    "        cat_features=[X_tr.columns.get_loc(col) for col in categorical_features if col in X_tr.columns],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pred = cat_model.predict_proba(X_val)[:, 1]\n",
    "    pred = np.asarray(pred, dtype=np.float32).ravel()\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    print(f\"    ‚úÖ CatBoost AUC: {auc:.5f} | Time: {(time.time() - fold_start) / 60:.2f} min\")\n",
    "\n",
    "    if auc > best_cat_auc:\n",
    "        best_cat_params = catboost_base_params.copy()\n",
    "        best_cat_auc = auc\n",
    "\n",
    "    catboost_oof_pred[val_idx] = pred\n",
    "\n",
    "    print(f\"‚è± Fold {fold} done in {(time.time() - fold_start)/60:.2f} min\")\n",
    "\n",
    "print(\"\\nüèÅ All folds completed\")\n",
    "print(f\"üü© Best LightGBM AUC: {best_lgb_auc:.5f}\")\n",
    "print(f\"üê± Best CatBoost AUC: {best_cat_auc:.5f}\")\n",
    "print(f\"‚è± Total time: {(time.time() - global_start)/60:.2f} min\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abeb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 10. TYPE-SAFE FINAL BASE MODEL TRAINING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüåü TYPE-SAFE FINAL BASE MODEL TRAINING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Train final LightGBM model\n",
    "if best_lgb_params is None:\n",
    "    raise ValueError(\"No best LightGBM parameters found during tuning\")\n",
    "train_lgb = lgb.Dataset(\n",
    "    X_train,\n",
    "    label=y_train.astype(np.int8),\n",
    "    categorical_feature=categorical_features,\n",
    "    free_raw_data=True\n",
    ")\n",
    "val_lgb = lgb.Dataset(\n",
    "    X_val,\n",
    "    label=y_val.astype(np.int8),\n",
    "    categorical_feature=categorical_features,\n",
    "    reference=train_lgb,\n",
    "    free_raw_data=True\n",
    ")\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    best_lgb_params,\n",
    "    train_lgb,\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=['train', 'eval'],\n",
    "    num_boost_round=1400,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train final CatBoost model\n",
    "if best_catboost_params is None:\n",
    "    raise ValueError(\"No best CatBoost parameters found during tuning\")\n",
    "catboost_model = CatBoostClassifier(**best_catboost_params)\n",
    "catboost_model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=200\n",
    ")\n",
    "# Base model predictions for evaluation\n",
    "lgb_train_pred = np.asarray(lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration), dtype=np.float64)\n",
    "lgb_val_pred = np.asarray(lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration), dtype=np.float64)\n",
    "catboost_train_pred = np.asarray(catboost_model.predict_proba(X_train)[:, 1], dtype=np.float64)\n",
    "catboost_val_pred = np.asarray(catboost_model.predict_proba(X_val)[:, 1], dtype=np.float64)\n",
    "val_oof_pred = np.column_stack((lgb_val_pred, catboost_val_pred))\n",
    "catboost_val_pred = np.asarray(catboost_model.predict_proba(X_val)[:, 1], dtype=np.float64)\n",
    "\n",
    "# AUC for base models\n",
    "lgb_train_auc = roc_auc_score(y_train, lgb_train_pred)\n",
    "lgb_val_auc = roc_auc_score(y_val, lgb_val_pred)\n",
    "catboost_train_auc = roc_auc_score(y_train, catboost_train_pred)\n",
    "catboost_val_auc = roc_auc_score(y_val, catboost_val_pred)\n",
    "\n",
    "print(f\"\\nüìä Base Model Performance:\")\n",
    "print(f\"  LightGBM Training AUC: {lgb_train_auc:.6f}\")\n",
    "print(f\"  LightGBM Validation AUC: {lgb_val_auc:.6f}\")\n",
    "print(f\"  CatBoost Training AUC: {catboost_train_auc:.6f}\")\n",
    "print(f\"  CatBoost Validation AUC: {catboost_val_auc:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c593b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 11. TYPE-SAFE META-MODEL TRAINING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüé≠ TYPE-SAFE META-MODEL TRAINING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare meta-features (OOF predictions)\n",
    "meta_train_features = np.column_stack((lgb_oof_pred, catboost_oof_pred))\n",
    "meta_val_features = np.column_stack((lgb_val_pred, catboost_val_pred))\n",
    "\n",
    "# Train logistic regression meta-model\n",
    "meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "meta_model.fit(meta_train_features, y_train)\n",
    "\n",
    "# Meta-model predictions\n",
    "meta_train_pred = np.asarray(meta_model.predict_proba(meta_train_features)[:, 1], dtype=np.float64)\n",
    "meta_val_pred = np.asarray(meta_model.predict_proba(meta_val_features)[:, 1], dtype=np.float64)\n",
    "\n",
    "# AUC for meta-model\n",
    "meta_train_auc = roc_auc_score(y_train, meta_train_pred)\n",
    "meta_val_auc = roc_auc_score(y_val, meta_val_pred)\n",
    "\n",
    "print(f\"\\nüìä Meta-Model Performance:\")\n",
    "print(f\"  Training AUC: {meta_train_auc:.6f}\")\n",
    "print(f\"  Validation AUC: {meta_val_auc:.6f}\")\n",
    "print(f\"  Overfitting gap: {meta_train_auc - meta_val_auc:.6f}\")\n",
    "\n",
    "# Performance comparison\n",
    "individual_aucs: List[float] = [float(lgb_val_auc), float(catboost_val_auc)]\n",
    "best_individual: float = max(individual_aucs)\n",
    "improvement: float = float(meta_val_auc - best_individual)\n",
    "\n",
    "print(f\"\\nüìà Model comparison summary:\")\n",
    "print(f\"  LightGBM solo: {lgb_val_auc:.6f}\")\n",
    "print(f\"  CatBoost solo: {catboost_val_auc:.6f}\")\n",
    "print(f\"  Meta-Model: {meta_val_auc:.6f}\")\n",
    "print(f\"  Best improvement: +{improvement:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447dbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 12. TYPE-SAFE FINAL TEST PREDICTIONS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüéØ GENERATING TYPE-SAFE FINAL TEST PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate test predictions from base models\n",
    "lgb_test_pred = np.asarray(lgb_test_pred, dtype=np.float64)\n",
    "catboost_test_pred = np.asarray(catboost_test_pred, dtype=np.float64)\n",
    "\n",
    "# Prepare meta-features for test set\n",
    "meta_test_features = np.column_stack((lgb_test_pred, catboost_test_pred))\n",
    "\n",
    "# Meta-model test predictions\n",
    "ensemble_test_pred: PredictionArray = np.asarray(meta_model.predict_proba(meta_test_features)[:, 1], dtype=np.float64)\n",
    "\n",
    "print(f\"üìä Test prediction statistics:\")\n",
    "print(f\"  Mean prediction: {float(ensemble_test_pred.mean()):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 13. SUBMISSION PREPARATION\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüìÑ PREPARING SUBMISSION FILES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Load original test file to maintain structure\n",
    "test_original = pd.read_csv('ctr_test.csv')\n",
    "test_original['click'] = ensemble_test_pred\n",
    "\n",
    "# Save updated test file\n",
    "test_original.to_csv('ctr_test.csv', index=False)\n",
    "print(\"‚úÖ Updated ctr_test.csv with ensemble predictions\")\n",
    "\n",
    "# Prepare submission file\n",
    "submission_df = pd.read_csv('ctr_sample_submission.csv')\n",
    "submission_mapping = dict(zip(test_original['idx'], ensemble_test_pred))\n",
    "\n",
    "submission_df['click'] = submission_df['idx'].map(submission_mapping)\n",
    "\n",
    "# Validate submission completeness\n",
    "missing_predictions = submission_df['click'].isna().sum()\n",
    "if missing_predictions > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {missing_predictions} missing predictions filled with mean\")\n",
    "    submission_df['click'].fillna(ensemble_test_pred.mean(), inplace=True)\n",
    "\n",
    "submission_df.to_csv('ctr_sample_submission.csv', index=False)\n",
    "print(\"‚úÖ Final submission file ready: ctr_sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 14. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# LightGBM feature importance\n",
    "lgb_importance = lgb_model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'lgb_importance': lgb_importance,\n",
    "    'catboost_importance': catboost_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Use meta-model coefficients for ensemble importance\n",
    "meta_coefficients = np.abs(meta_model.coef_[0])\n",
    "feature_importance_df['ensemble_importance'] = (\n",
    "    meta_coefficients[0] * feature_importance_df['lgb_importance'] +\n",
    "    meta_coefficients[1] * feature_importance_df['catboost_importance']\n",
    ")\n",
    "\n",
    "top_features = feature_importance_df.nlargest(15, 'ensemble_importance')\n",
    "\n",
    "print(\"üèÜ Top 15 most important features:\")\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:<25} ‚Üí {row['ensemble_importance']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 15. FINAL PERFORMANCE SUMMARY\n",
    "# =============================================\n",
    "\n",
    "print(f\"\\n{'üéâ FINAL PERFORMANCE SUMMARY'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "expected_points = 100 * max(0.0, float(meta_val_auc - 0.60)) / 0.40\n",
    "\n",
    "print(f\"üìä Validation Results:\")\n",
    "print(f\"  Final Meta-Model AUC: {meta_val_auc:.6f}\")\n",
    "print(f\"  Expected Competition Points: {expected_points:.1f}/100\")\n",
    "\n",
    "if meta_val_auc >= 0.80:\n",
    "    print(\"üèÜ EXCEPTIONAL RESULT! Target AUC ‚â• 0.80 achieved\")\n",
    "elif meta_val_auc >= 0.75:\n",
    "    print(\"üéØ EXCELLENT RESULT! Strong competitive performance\")\n",
    "elif meta_val_auc >= 0.70:\n",
    "    print(\"‚úÖ SOLID RESULT! Significant improvement achieved\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è MODERATE IMPROVEMENT. Consider additional feature engineering\")\n",
    "\n",
    "print(f\"\\nüîß Key improvements implemented:\")\n",
    "print(f\"  ‚úÖ Temporal 5-fold cross-validation with TimeSeriesSplit\")\n",
    "print(f\"  ‚úÖ Frequency encoding with Laplace smoothing\")\n",
    "print(f\"  ‚úÖ CTR-based aggregation features with leakage checks\")\n",
    "print(f\"  ‚úÖ Advanced temporal feature engineering (simplified)\")\n",
    "print(f\"  ‚úÖ LightGBM + CatBoost with stacking via logistic regression meta-model\")\n",
    "print(f\"  ‚úÖ Hyperparameter tuning for base models\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for submission! Expected significant improvement in leaderboard AUC.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
